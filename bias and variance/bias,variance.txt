BIAS AND VARIANCE : Bias and Varaince terms comes into picture only when we build the model , or when we have training and test data.
                  
                  Consider a scenario,where we have x and y axis,and on that graph points are scattered and some problem statement is given and i am using one of linear regression
                  technique: Linear regression is given by the equation y=mx+c , and graphically it is represented by term best fit line.
                  
NOTE :            Points are scattered randomly.

                  Now we have have training data and test data.
                  
                  Coming to the training data set
                  I will try to fit the linear regression(or the best fit line).
                  
NOTE :            Best fit line cannot fit all the training points,as they are scattered one. Thus, the straight line will never capture true datapoints of training data                             no matter how well we fit it.

BIAS :            The failure to capture true points in x and y plane is called bias.And the distance of a particular point to the fit line is called error.

NOTE :            Coming to the testing points plotting with the best fit line we just calculated.Here, most of the points fit the best fit line and some points will still remain

VARIANCE :        The difference of the fit between training and test data is called variance. (the spread of the predicted data is called variance).




CONDITIONS OF OVERFITTING AND UNDERFITTING :

FOR REGRESSION TECHNIQUES :

When training set is giving me high error(r^2 value)            When training set accuracy is high and test                  When both the data set are well balanced.
it is called underfitting.                                      accuracy is low .

HIGH BIAS AND HIGH VARIANCE                                     LOW BIAS AND HIGH VARIANCE                                    LOW BIAS AND LOW VARIANCE




