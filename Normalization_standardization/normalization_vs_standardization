Q) Which algorithm does not require scaling??
ANS) Decision tree, random forest (these are bagging technique), all type of boosters.
Because they itself makes small branches.



It is observed that standardization is much more effective than normalization.
NORMALIZATION DOSEN'T TREAT OUTLIERS WELL.
WHEREAS STANDARDIZATION SCALES DOWN PERFECTLY.

Does this means normalization is bad, no particularly, it depends on the data set, when the data set is small one can go with normalization.


It is an important step in feature scaling which is an integral part of feature engineering.

Normalisation : It helps to scale down the dataset in the range of 0 to 1. (also called min max scalar).
                 There are variety of normalization, but i have just used min max scaler.
                 
Normal distribution follows : 68 : 95 : 99.5 rule for distribution                 

                  X(norm)=    X-X(min)
                            ------------  
                            X(max)-X(min)


Q)How to import it??
ANS)from sklearn.preprocessing import MinMaxScaler
    scaling=MinMaxScaler()
    scaling.fit_transform(df[["","",""]])                                    df=dataset name                   in square bracket put feature name.                                                
    
    
                  

Standardisation : It helps to scale down your feature based on the standard normal distribution( mean = 0 and standard deviation = 1 )(also called z score transformation).

                  z=  x- μ
                     ------
                       σ
                       
                        μ=meu=mean
                        σ=sigma=standard deviation
                        x=feature data


Q) How to import it?
ANS) from sklearn.preprocessing import StandardScaler
     scaling=StandardScaler()
     scaling.fit_transform(df[["","",""]])                                       df=dataset name                   in square bracket put feature name.
    
    
    
    
    
 Q) When to use standardization and when to normalization??
 ANS) Max-Min Normalisation typically allows us to transform the data with varying scales so that no specific dimension will dominate the statistics,
      and it does not require making a very strong assumption about the distribution of the data, such as k-nearest neighbours and artificial neural networks.
      However, Normalisation does not treat outliners very well. On the contrary, standardisation allows users to better handle the outliers and facilitate
      convergence for some computational algorithms like gradient descent. Therefore, we usually prefer standardisation over Min-Max Normalisation.

Q) What algorithm needs feature scaling?
ANS) k means               = it uses euclidean distance
     knn                   = measure the distance between the samples(kvalue).
     PCA                   = variance concept is used (it obtaines the feature with maximum variance).
     Gradient descent      = required
     
     
