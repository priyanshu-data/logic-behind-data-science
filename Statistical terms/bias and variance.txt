BIAS AND VARIANCE : Bias and Varaince terms comes into picture only when we build the model , or when we have training and test data. or to put simply put, bias is training                         bias is training error and variance is testing error.
                  
                  Consider a scenario,where we have x and y axis,and on that graph points are scattered and some problem statement is given and i am using one of linear regression
                  technique: Linear regression is given by the equation y=mx+c , and graphically it is represented by term best fit line.
                  
NOTE :            Points are scattered randomly.

                  Now we have have training data and test data.
                  
                  Coming to the training data set
                  I will try to fit the linear regression(or the best fit line).
                  
NOTE :            Best fit line cannot fit all the training points,as they are scattered one. Thus, the straight line will never capture true datapoints of training data                             no matter how well we fit it.

BIAS :            The failure to capture true points in x and y plane is called bias.And the distance of a particular point to the fit line is called error.

NOTE :            Coming to the testing points plotting with the best fit line we just calculated.Here, most of the points fit the best fit line and some points will still remain

VARIANCE :        The difference of the fit between training and test data is called variance. (the spread of the predicted data is called variance).




CONDITIONS OF OVERFITTING AND UNDERFITTING :

FOR REGRESSION TECHNIQUES :

When training set is giving me high error(r^2 value)            When training set accuracy is high and test                  When both the data set are well balanced.
and testing error is also high                                  accuracy is low .

HIGH BIAS AND HIGH VARIANCE                                     LOW BIAS AND HIGH VARIANCE                                   LOW BIAS AND LOW VARIANCE

it is called underfitting.                                       Overfitting                                                 Well balanced.



FOR CLASSIFICATION TECHNIQUES :

When training error is low and testing error is high.           When training error is high and test error                   When both are having less error.  
                                                                is high.
LOW BIAS  AND HIGH VARIANCE                                     HIGH BIAS AND HIGH VARIANCE                                  LOW BIAS AND LOW VARIANCE

The model is overfitting.                                       The model is underfitting.                                   MODEL IS WELL BALANCED.

